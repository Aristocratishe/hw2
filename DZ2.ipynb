{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "XQY5EH7ZxlNw"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import PIL\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.core.display import Image, display\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "torch.manual_seed(2)\n",
    "np.random.seed(2)\n",
    "\n",
    "# правильно выбирайте карту, иначе все упадет!\n",
    "CUDA_DEVICE=3\n",
    "\n",
    "def data2image(data):\n",
    "    res = np.transpose(np.reshape(data, (3, 32, 32)), (1, 2, 0))\n",
    "    return PIL.Image.fromarray(np.uint8(res))\n",
    "\n",
    "def imshow(img):\n",
    "    npimg = img.numpy().astype('uint8')\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr  4 16:03:00 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.43       Driver Version: 418.43       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 108...  Off  | 00000000:02:00.0 Off |                  N/A |\n",
      "| 23%   29C    P8     9W / 250W |     10MiB / 11178MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce GTX 108...  Off  | 00000000:03:00.0 Off |                  N/A |\n",
      "| 23%   26C    P8     8W / 250W |     10MiB / 11178MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  GeForce GTX 108...  Off  | 00000000:81:00.0 Off |                  N/A |\n",
      "| 23%   25C    P8     8W / 250W |     10MiB / 11178MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  GeForce GTX 108...  Off  | 00000000:82:00.0 Off |                  N/A |\n",
      "| 34%   47C    P8    10W / 250W |   5163MiB / 11178MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    3      5682      C   /home/d.devyatkin/anaconda3/bin/python      5153MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "332yVfwPxlN3"
   },
   "outputs": [],
   "source": [
    "class CustomC100Dataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, dataset, dataset_type, img_cnt = 50000, transform = None, target_transform = None):\n",
    "\n",
    "        if dataset_type not in ['train', 'test']:\n",
    "            raise \"Unknown dataset type : {}\".format(dataset_type)\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.ds_type = dataset_type\n",
    "        self.img_cnt = img_cnt\n",
    "        self.transform = transform\n",
    "        self.t_transform = target_transform\n",
    "        self.__load__()\n",
    "    \n",
    "    def __load__(self):\n",
    "\n",
    "        if self.ds_type == 'train':\n",
    "            dataset = self.dataset\n",
    "            dataset = dataset.reshape((self.img_cnt, 3073))\n",
    "            self.y, self.x = np.hsplit(dataset, [1])\n",
    "            self.y = self.y.astype(np.int64)\n",
    "            self.x = self.x.reshape((self.x.shape[0], 3, 32, 32))\n",
    "            self.x = self.x.transpose((0, 2, 3, 1))\n",
    "        \n",
    "        if self.ds_type == 'test':\n",
    "            dataset = self.dataset.reshape((self.img_cnt, 3072))\n",
    "            self.y = np.zeros((dataset.shape[0], 1), dtype=np.int64)\n",
    "            self.x = dataset.reshape((dataset.shape[0], 3, 32, 32))\n",
    "            self.x = self.x.transpose((0, 2, 3, 1))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        img, target = self.x[index], self.y[index]\n",
    "        \n",
    "        img = PIL.Image.fromarray(img)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.t_transform is not None:\n",
    "            target = self.t_transform(target)\n",
    "            \n",
    "        return img, target\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "17zYBNzlxlN5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose(\n",
    "    [\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomHorizontalFlip(p=0.8),\n",
    "    transforms.RandomVerticalFlip(p=0.1),\n",
    "    transforms.RandomRotation(5),\n",
    "    transforms.RandomAffine(5),\n",
    "   # transforms.Pad(4)\n",
    "    transforms.ColorJitter(brightness=0.5, contrast=0.8, saturation=0.3, hue=0.01),\n",
    "    transforms.Normalize([0, 0, 0], [1, 1, 1])\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0, 0, 0], [1, 1, 1])\n",
    "])\n",
    "\n",
    "# подумайте, должны ли преобразования train и test сет быть абсолютно одинаковыми ?\n",
    "\n",
    "dataset_train = np.load('homework_2.train.npy')\n",
    "\n",
    "valid = np.load('homework_2_no_classes.test.npy')\n",
    "\n",
    "np.random.seed(42)  \n",
    "\n",
    "idx = list(range(50000))\n",
    "\n",
    "sh = np.random.shuffle(idx)\n",
    "\n",
    "train, test = dataset_train[idx[:45000]], dataset_train[idx[45000:]]\n",
    "\n",
    "train_dataset = CustomC100Dataset(train, 'train',\n",
    "                                  img_cnt = 45000, \n",
    "                                  transform = transform_train)\n",
    "\n",
    "test_dataset = CustomC100Dataset(test, 'train',\n",
    "                                  img_cnt = 5000, \n",
    "                                  transform = transform_test)\n",
    "\n",
    "validation_dataset = CustomC100Dataset(valid, 'test',\n",
    "                                    img_cnt = 10000,\n",
    "                                    transform = transform_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, lbl = train_dataset[5]\n",
    "imshow(img * 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "nKErbWV2xlN7"
   },
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    \n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "\n",
    "        self.conv2d_1 = torch.nn.Conv2d(inplanes,\n",
    "                                        planes,\n",
    "                                        kernel_size=(3, 3),\n",
    "                                        stride=stride, \n",
    "                                        padding=1,\n",
    "                                        bias=False)\n",
    "        \n",
    "        self.bn_1 = torch.nn.BatchNorm2d(planes)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.conv2d_2 = torch.nn.Conv2d(planes,\n",
    "                                  planes,\n",
    "                                  kernel_size=(3, 3), \n",
    "                                  stride=stride, \n",
    "                                  padding=1,\n",
    "                                  bias=False)\n",
    "        \n",
    "        self.bn_2 = torch.nn.BatchNorm2d(planes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv2d_1(x)\n",
    "        x = self.bn_1(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        x = self.conv2d_2(x)\n",
    "        x = self.bn_2(x)\n",
    "\n",
    "        return self.activation(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleNeck(nn.Module):\n",
    "    expansion = 4\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv2d_1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn_1 = nn.BatchNorm2d(planes)\n",
    "        \n",
    "        self.activation = torch.nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.conv2d_2 = nn.Conv2d(planes, planes, stride=stride, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn_2 = nn.BatchNorm2d(planes)\n",
    "        \n",
    "        self.conv2d_3 = nn.Conv2d(planes, planes * BottleNeck.expansion, kernel_size=1, bias=False)\n",
    "        self.bn_3 = nn.BatchNorm2d(planes * BottleNeck.expansion)\n",
    "        \n",
    "        self.downsample = nn.Sequential()\n",
    "        \n",
    "        if stride != 1 or inplanes != planes * BottleNeck.expansion:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(inplanes, planes * BottleNeck.expansion, stride=stride, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(planes * BottleNeck.expansion)\n",
    "            )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        y = self.downsample(x)\n",
    "        \n",
    "        x = self.conv2d_1(x)\n",
    "        x = self.bn_1(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        x = self.conv2d_2(x)\n",
    "        x = self.bn_2(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        x = self.conv2d_3(x)\n",
    "        x = self.bn_3(x)\n",
    "        \n",
    "        return self.activation(x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "4eyJp4avxlN8"
   },
   "outputs": [],
   "source": [
    "# чтобы вы не запутались, сборку самой модели предоставляем\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=100):\n",
    "        \n",
    "        super(ResNet, self).__init__()\n",
    "        self.inplanes = 16\n",
    "        self.conv1 = torch.nn.Conv2d(3, 16, kernel_size=(3, 3), stride=1, padding=1, bias=False)\n",
    "        self.bn1 = torch.nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self._make_layer(block, 16, layers[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 32, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, layers[2], stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(8)\n",
    "\n",
    "        self.fc = nn.Linear(64 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "#         if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "#             downsample = nn.Sequential(\n",
    "#                 nn.Conv2d(self.inplanes, block.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "#                 nn.BatchNorm2d(block.expansion * planes)\n",
    "#             )\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        # print(\"[conv1]\", x.shape)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        # print(\"[layer1]\", x.shape)\n",
    "        x = self.layer2(x)\n",
    "        # print(\"[layer2]\", x.shape)\n",
    "        x = self.layer3(x)\n",
    "        # print(\"[layer3]\", x.shape)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        # print(\"[avgpool]\", x.shape)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выбирайте любой ResNet\n",
    "\n",
    "def ResNet20():\n",
    "    return ResNet(BottleNeck, [3, 3, 3], 100)\n",
    "\n",
    "def ResNet32():\n",
    "    return ResNet(BottleNeck, [5, 5, 5], 100)\n",
    "\n",
    "def ResNet44():\n",
    "    return ResNet(BottleNeck, [7, 7, 7], 100)\n",
    "\n",
    "def ResNet56():\n",
    "    return ResNet(BottleNeck, [9, 9, 9], 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "rgOA0HHBxlN-"
   },
   "outputs": [],
   "source": [
    "def acc_check(model, validation_loader, batch_size):\n",
    "    model.eval()\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    \n",
    "    for batch_idx, (inputs, labels) in enumerate(validation_loader):\n",
    "        inputs, labels = inputs.cuda(CUDA_DEVICE), labels.view(batch_size).cuda(CUDA_DEVICE)\n",
    "        outputs = model(inputs)\n",
    "        outputs = F.softmax(outputs)\n",
    "        prob, predicted = torch.topk(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.view(-1).eq(labels.data).sum()\n",
    "        \n",
    "    model.train()\n",
    "    return (correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "WPUL4G7KxlN_"
   },
   "outputs": [],
   "source": [
    "def make_solution_pytorch(model, test_data_loader, batch_size):\n",
    "    res = []\n",
    "    model.eval()\n",
    "    for batch_idx, (inputs, labels) in enumerate(test_data_loader):\n",
    "        inputs, labels = inputs.cuda(CUDA_DEVICE), labels.view(batch_size).cuda(CUDA_DEVICE)\n",
    "        outputs = model(inputs)\n",
    "        outputs = F.softmax(outputs)\n",
    "        prob, predicted = torch.topk(outputs.data, 1)\n",
    "        res = np.append(res, predicted.view(-1).cpu().numpy())\n",
    "    model.train()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write2file(path, data, i):\n",
    "    types: str = 'w' if not i else 'a'\n",
    "    with open(path, types) as f:\n",
    "        f.write(str(data.cpu().numpy()) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "5ylX1WFyitOY",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = ResNet56().cuda(CUDA_DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr  4 16:04:13 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.43       Driver Version: 418.43       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 108...  Off  | 00000000:02:00.0 Off |                  N/A |\n",
      "| 23%   29C    P8     9W / 250W |     10MiB / 11178MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce GTX 108...  Off  | 00000000:03:00.0 Off |                  N/A |\n",
      "| 23%   26C    P8     8W / 250W |     10MiB / 11178MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  GeForce GTX 108...  Off  | 00000000:81:00.0 Off |                  N/A |\n",
      "| 23%   25C    P8     8W / 250W |     10MiB / 11178MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  GeForce GTX 108...  Off  | 00000000:82:00.0 Off |                  N/A |\n",
      "| 24%   35C    P2    55W / 250W |   5163MiB / 11178MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    3      5682      C   /home/d.devyatkin/anaconda3/bin/python      5153MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-01.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d992ef9e54644f53ac432d8d97ebed36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=110.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/d.devyatkin/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################### \n",
      " 1 / 110 \n",
      " acc - 0.019999999552965164\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "################################### \n",
      " 2 / 110 \n",
      " acc - 0.02539999969303608\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "################################### \n",
      " 3 / 110 \n",
      " acc - 0.05199999734759331\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "################################### \n",
      " 4 / 110 \n",
      " acc - 0.0722000002861023\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "################################### \n",
      " 5 / 110 \n",
      " acc - 0.1005999967455864\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "################################### \n",
      " 6 / 110 \n",
      " acc - 0.121799997985363\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "################################### \n",
      " 7 / 110 \n",
      " acc - 0.1501999944448471\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "################################### \n",
      " 8 / 110 \n",
      " acc - 0.18239998817443848\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "################################### \n",
      " 9 / 110 \n",
      " acc - 0.22280000150203705\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "################################### \n",
      " 10 / 110 \n",
      " acc - 0.2847999930381775\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "################################### \n",
      " 11 / 110 \n",
      " acc - 0.30059999227523804\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "################################### \n",
      " 12 / 110 \n",
      " acc - 0.3465999960899353\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/d.devyatkin/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################### \n",
      " 13 / 110 \n",
      " acc - 0.36820000410079956\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "################################### \n",
      " 14 / 110 \n",
      " acc - 0.3813999891281128\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "################################### \n",
      " 15 / 110 \n",
      " acc - 0.39559999108314514\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "################################### \n",
      " 16 / 110 \n",
      " acc - 0.42980000376701355\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "################################### \n",
      " 17 / 110 \n",
      " acc - 0.44279998540878296\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "################################### \n",
      " 18 / 110 \n",
      " acc - 0.4490000009536743\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "################################### \n",
      " 19 / 110 \n",
      " acc - 0.47039997577667236\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "################################### \n",
      " 20 / 110 \n",
      " acc - 0.48339998722076416\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "################################### \n",
      " 21 / 110 \n",
      " acc - 0.4861999750137329\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "################################### \n",
      " 22 / 110 \n",
      " acc - 0.5037999749183655\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "################################### \n",
      " 23 / 110 \n",
      " acc - 0.5067999958992004\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "################################### \n",
      " 24 / 110 \n",
      " acc - 0.5108000040054321\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "################################### \n",
      " 25 / 110 \n",
      " acc - 0.5091999769210815\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "################################### \n",
      " 26 / 110 \n",
      " acc - 0.5217999815940857\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "################################### \n",
      " 27 / 110 \n",
      " acc - 0.5295999646186829\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "################################### \n",
      " 28 / 110 \n",
      " acc - 0.5191999673843384\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "################################### \n",
      " 29 / 110 \n",
      " acc - 0.527999997138977\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "################################### \n",
      " 30 / 110 \n",
      " acc - 0.5442000031471252\n",
      "Adjusting learning rate of group 0 to 2.0000e-02.\n",
      "################################### \n",
      " 31 / 110 \n",
      " acc - 0.5920000076293945\n",
      "Adjusting learning rate of group 0 to 2.0000e-02.\n",
      "################################### \n",
      " 32 / 110 \n",
      " acc - 0.5974000096321106\n",
      "Adjusting learning rate of group 0 to 2.0000e-02.\n",
      "################################### \n",
      " 33 / 110 \n",
      " acc - 0.590999960899353\n",
      "Adjusting learning rate of group 0 to 2.0000e-02.\n",
      "################################### \n",
      " 34 / 110 \n",
      " acc - 0.6037999987602234\n",
      "Adjusting learning rate of group 0 to 2.0000e-02.\n",
      "################################### \n",
      " 35 / 110 \n",
      " acc - 0.590399980545044\n",
      "Adjusting learning rate of group 0 to 2.0000e-02.\n",
      "################################### \n",
      " 36 / 110 \n",
      " acc - 0.592199981212616\n",
      "Adjusting learning rate of group 0 to 2.0000e-02.\n",
      "################################### \n",
      " 37 / 110 \n",
      " acc - 0.5959999561309814\n",
      "Adjusting learning rate of group 0 to 2.0000e-02.\n",
      "################################### \n",
      " 38 / 110 \n",
      " acc - 0.5941999554634094\n",
      "Adjusting learning rate of group 0 to 2.0000e-02.\n",
      "################################### \n",
      " 39 / 110 \n",
      " acc - 0.5941999554634094\n",
      "Adjusting learning rate of group 0 to 2.0000e-02.\n",
      "################################### \n",
      " 40 / 110 \n",
      " acc - 0.5945999622344971\n",
      "Adjusting learning rate of group 0 to 2.0000e-02.\n",
      "################################### \n",
      " 41 / 110 \n",
      " acc - 0.5934000015258789\n",
      "Adjusting learning rate of group 0 to 2.0000e-02.\n",
      "################################### \n",
      " 42 / 110 \n",
      " acc - 0.5902000069618225\n",
      "Adjusting learning rate of group 0 to 2.0000e-02.\n",
      "################################### \n",
      " 43 / 110 \n",
      " acc - 0.5949999690055847\n",
      "Adjusting learning rate of group 0 to 2.0000e-02.\n",
      "################################### \n",
      " 44 / 110 \n",
      " acc - 0.5956000089645386\n",
      "Adjusting learning rate of group 0 to 2.0000e-02.\n",
      "################################### \n",
      " 45 / 110 \n",
      " acc - 0.597000002861023\n",
      "Adjusting learning rate of group 0 to 2.0000e-02.\n",
      "################################### \n",
      " 46 / 110 \n",
      " acc - 0.5888000130653381\n",
      "Adjusting learning rate of group 0 to 2.0000e-02.\n",
      "################################### \n",
      " 47 / 110 \n",
      " acc - 0.5870000123977661\n",
      "Adjusting learning rate of group 0 to 2.0000e-02.\n",
      "################################### \n",
      " 48 / 110 \n",
      " acc - 0.5965999960899353\n",
      "Adjusting learning rate of group 0 to 2.0000e-02.\n",
      "################################### \n",
      " 49 / 110 \n",
      " acc - 0.5956000089645386\n",
      "Adjusting learning rate of group 0 to 2.0000e-02.\n",
      "################################### \n",
      " 50 / 110 \n",
      " acc - 0.5961999893188477\n",
      "Adjusting learning rate of group 0 to 4.0000e-03.\n",
      "################################### \n",
      " 51 / 110 \n",
      " acc - 0.6029999852180481\n",
      "Adjusting learning rate of group 0 to 4.0000e-03.\n",
      "################################### \n",
      " 52 / 110 \n",
      " acc - 0.6025999784469604\n",
      "Adjusting learning rate of group 0 to 4.0000e-03.\n",
      "################################### \n",
      " 53 / 110 \n",
      " acc - 0.6067999601364136\n",
      "Adjusting learning rate of group 0 to 4.0000e-03.\n",
      "################################### \n",
      " 54 / 110 \n",
      " acc - 0.6007999777793884\n",
      "Adjusting learning rate of group 0 to 4.0000e-03.\n",
      "################################### \n",
      " 55 / 110 \n",
      " acc - 0.5999999642372131\n",
      "Adjusting learning rate of group 0 to 4.0000e-03.\n",
      "################################### \n",
      " 56 / 110 \n",
      " acc - 0.5983999967575073\n",
      "Adjusting learning rate of group 0 to 4.0000e-03.\n",
      "################################### \n",
      " 57 / 110 \n",
      " acc - 0.6037999987602234\n",
      "Adjusting learning rate of group 0 to 4.0000e-03.\n",
      "################################### \n",
      " 58 / 110 \n",
      " acc - 0.6010000109672546\n",
      "Adjusting learning rate of group 0 to 4.0000e-03.\n",
      "################################### \n",
      " 59 / 110 \n",
      " acc - 0.6033999919891357\n",
      "Adjusting learning rate of group 0 to 4.0000e-03.\n",
      "################################### \n",
      " 60 / 110 \n",
      " acc - 0.5977999567985535\n",
      "Adjusting learning rate of group 0 to 4.0000e-03.\n",
      "################################### \n",
      " 61 / 110 \n",
      " acc - 0.6003999710083008\n",
      "Adjusting learning rate of group 0 to 4.0000e-03.\n",
      "################################### \n",
      " 62 / 110 \n",
      " acc - 0.6007999777793884\n",
      "Adjusting learning rate of group 0 to 4.0000e-03.\n",
      "################################### \n",
      " 63 / 110 \n",
      " acc - 0.6015999913215637\n",
      "Adjusting learning rate of group 0 to 4.0000e-03.\n",
      "################################### \n",
      " 64 / 110 \n",
      " acc - 0.602400004863739\n",
      "Adjusting learning rate of group 0 to 4.0000e-03.\n",
      "################################### \n",
      " 65 / 110 \n",
      " acc - 0.6031999588012695\n",
      "Adjusting learning rate of group 0 to 4.0000e-03.\n",
      "################################### \n",
      " 66 / 110 \n",
      " acc - 0.6015999913215637\n",
      "Adjusting learning rate of group 0 to 4.0000e-03.\n",
      "################################### \n",
      " 67 / 110 \n",
      " acc - 0.5999999642372131\n",
      "Adjusting learning rate of group 0 to 4.0000e-03.\n",
      "################################### \n",
      " 68 / 110 \n",
      " acc - 0.5977999567985535\n",
      "Adjusting learning rate of group 0 to 4.0000e-03.\n",
      "################################### \n",
      " 69 / 110 \n",
      " acc - 0.6067999601364136\n",
      "Adjusting learning rate of group 0 to 4.0000e-03.\n",
      "################################### \n",
      " 70 / 110 \n",
      " acc - 0.6019999980926514\n",
      "Adjusting learning rate of group 0 to 8.0000e-04.\n",
      "################################### \n",
      " 71 / 110 \n",
      " acc - 0.604200005531311\n",
      "Adjusting learning rate of group 0 to 8.0000e-04.\n",
      "################################### \n",
      " 72 / 110 \n",
      " acc - 0.6039999723434448\n",
      "Adjusting learning rate of group 0 to 8.0000e-04.\n",
      "################################### \n",
      " 73 / 110 \n",
      " acc - 0.6035999655723572\n",
      "Adjusting learning rate of group 0 to 8.0000e-04.\n",
      "################################### \n",
      " 74 / 110 \n",
      " acc - 0.6043999791145325\n",
      "Adjusting learning rate of group 0 to 8.0000e-04.\n",
      "################################### \n",
      " 75 / 110 \n",
      " acc - 0.602400004863739\n",
      "Adjusting learning rate of group 0 to 8.0000e-04.\n",
      "################################### \n",
      " 76 / 110 \n",
      " acc - 0.6051999926567078\n",
      "Adjusting learning rate of group 0 to 8.0000e-04.\n",
      "################################### \n",
      " 77 / 110 \n",
      " acc - 0.6035999655723572\n",
      "Adjusting learning rate of group 0 to 8.0000e-04.\n",
      "################################### \n",
      " 78 / 110 \n",
      " acc - 0.6017999649047852\n",
      "Adjusting learning rate of group 0 to 8.0000e-04.\n",
      "################################### \n",
      " 79 / 110 \n",
      " acc - 0.6019999980926514\n",
      "Adjusting learning rate of group 0 to 8.0000e-04.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################### \n",
      " 80 / 110 \n",
      " acc - 0.6010000109672546\n",
      "Adjusting learning rate of group 0 to 1.6000e-04.\n"
     ]
    }
   ],
   "source": [
    "#Обучение ResNet\n",
    "#Не забудьте вывести графики accuracy (можно использовать acc_check()) и loss при обучении\n",
    "\n",
    "#all parameters from https://arxiv.org/pdf/1512.03385.pdf\n",
    "epochs_cnt = 110 # кол-во эпох\n",
    "\n",
    "# эти параметры тоже на ваше усмотрение\n",
    "minibatch_size = 200\n",
    "test_batch_size = 100\n",
    "\n",
    "#optimizer = optim.Adam(model.parameters(), lr=1e-3, betas=(0.8, 0.999))\n",
    "optimizer = optim.SGD(model.parameters(), momentum=0.9, lr=1e-1, nesterov=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().cuda(CUDA_DEVICE)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30, 50, 70, 80, 90], gamma=0.2, verbose=True)\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = test_batch_size, shuffle = False, num_workers = 2)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size = test_batch_size, shuffle = False, num_workers = 2)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = minibatch_size, shuffle = True, num_workers = 2)\n",
    "losses = []\n",
    "acc = []\n",
    "\n",
    "# примерный код для обучения модели, не обязательно строго следовать ему\n",
    "for epoch in tqdm(range(epochs_cnt)):    \n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        \n",
    "        if batch_idx % 10 == 0 and batch_idx:\n",
    "            losses.append(train_loss / 10)\n",
    "            train_loss = 0\n",
    "        \n",
    "        inputs, labels = inputs.cuda(CUDA_DEVICE), labels.view(minibatch_size).cuda(CUDA_DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels).cuda(CUDA_DEVICE)\n",
    "        losses.append(loss.data)\n",
    "        train_loss += loss.data\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    acc.append(acc_check(model, test_loader, test_batch_size))\n",
    "    print(f'################################### \\n {epoch + 1} / {epochs_cnt} \\n acc - {acc[-1]}')\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    if epoch > 10 and acc[-1] > max(acc[:-1]):\n",
    "        torch.save(model.state_dict(), \"model.pth\")\n",
    "        \n",
    "        solution = make_solution_pytorch(model, validation_loader, test_batch_size)\n",
    "        with open('result.csv', 'w') as fout:\n",
    "            print('Id', 'Prediction', sep=',', file=fout)\n",
    "            for i, prediction in enumerate(solution):\n",
    "                print(i, int(prediction), sep=',', file=fout)\n",
    "    \n",
    "    if acc[-1] > 0.68:\n",
    "        break\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, figsize=(16, 8))\n",
    "fig.suptitle(f\"max accuracy is {max([i.cpu().numpy() for i in acc])}\")\n",
    "axs[0].semilogy([i.cpu().numpy() for i in losses])\n",
    "axs[1].plot([i.cpu().numpy() for i in acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WgS9u4WjbAJM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Homework_2_Clear.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
